% \documentclass[11pt]{article}
%\documentclass[multi={mymath}, border=1pt]{standalone}
\documentclass[multi={mymath},border=1pt,convert={convertexe={convert},density=300,size=800x800,outext=.png}]{standalone}
\newenvironment{mymath}{$\displaystyle}{$}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{amscd}
\usepackage{mathrsfs}
\usepackage{verbatim}           
\usepackage{enumerate}

%%%%%% BASIC NOTATIONS %%%%%%

% 1. Number fields
\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\Rplus{{\mathbb{R}_+}}

% 2. Function spaces
\def\L2{{L^2(\mathbb{R})}}

% 2. Probability
\def\Pb{{\mathbb{P}}}           % probability measure
\def\Var{{\mathrm{Var}}}        % variance
\def\E{{\mathbb{E}}}            % mathematical expectation
\def\Borel{{\mathscr{B}}}       % Borel algebra
\def\F{ {\mathcal{F}} }         % sigma algebra

% 3. Symbols
\def\eps{\varepsilon}
\def\Cov{ {\textrm{Cov}} }
\def\Cop{ {\mathcal{C}} }
\def\I{ {\mathcal{I}} }
\def\D{ {\mathcal{D}} }
\def\O{ {\mathcal{O}} }
\def\U{ {\mathcal{U}} }
\def\L{ {\mathcal{L}} }
\def\B{ {\mathcal{B}} }
\def\G{ {\mathcal{G}} }
\def\T{ {\mathcal{T}} }
\def\M{ {\mathcal{M}} }
\def\S{ {\mathcal{S}} }
\def\lft{\mathscr{L}}
\def\astar{ {a^*} }
\def\Ct{ {\mathcal{C}} }
\def\Null{ {\mathcal{N}} }
\def\Norm{ {\mathcal{N}} }
\def\Range{ {\mathcal{R}} }
\def\K{ {\mathcal{K}} }
\def\H{ {\mathcal{H}} }
\def\bx{ {\boldsymbol x} }
\def\bp{ {\boldsymbol p} }
\def\bxi{ {\boldsymbol\xi} }
\def\bzeta{ {\boldsymbol\zeta} }
\def\|{{|\!|}}
\def\ac{{ \,<\!\!<\, }}
\def\<{{\langle}}
\def\>{{\rangle}}
\def\supp{{\mathrm{supp}}}
\def\yobs{ {y_{\textrm{obs}}} }

\newcommand{\diag}{\operatornamewithlimits{diag}}


\begin{document}

% 1. Simulation to hydro-field 
\begin{mymath}
  G(y) \mapsto H
\end{mymath}

% 2. Emulator network
\begin{mymath}
  \hat{G}_{\psi}(y) \approx G(y)
\end{mymath}

% 3. Simulation training set
\begin{mymath}
  \{(y^{(i)}, H^{(i)})\}_{i=1}^N
\end{mymath}

% 4. Emulator loss
\begin{mymath}
  L^{\text{G}}(\psi) = \frac{1}{N} \sum_{i=1}^N \text{Error}(\hat{G}_\psi(y^{(i)}), H^{(i)})
\end{mymath}

% 5. Geometry vector
\begin{mymath}
  y_t \in \mathbb{R}^n \textrm{ : Input vector at time } t
\end{mymath}

% 6. Emulated density field
\begin{mymath}
  H_t = \hat{G}(y_t) \textrm{ : Emulated field}
\end{mymath}

% 7. Target field
\begin{mymath}
  H^* \textrm{ : Target field}
\end{mymath}

% 8. Action on state space
\begin{mymath}
  x_t \textrm{ : Action taken at time } t \textrm{, representing an update to } y_t
\end{mymath}

% 9. State transition
\begin{mymath}
  y_{t+1} = y_t + x_t \textrm{ : State transition induced by action}
\end{mymath}

% 10. Reward
\begin{mymath}
  r_t = -\text{Error}(H_t, H^*) \textrm{ : Reward at time } t
\end{mymath}

% 11. Policy network
\begin{mymath}
  \pi_\theta(x_t \mid y_t, H_t, H^*)
\end{mymath}

% 12. Value network
\begin{mymath}
  V_\phi(y_t, H_t, H^*)
\end{mymath}

% % 
% \begin{mymath}
% \end{mymath}

% % 
% \begin{mymath}
% \end{mymath}

% % 
% \begin{mymath}
% \end{mymath}

% % 
% \begin{mymath}
% \end{mymath}

% % 
% \begin{mymath}
% \end{mymath}

% % 
% \begin{mymath}
% \end{mymath}

% The agent learns a policy \( \pi_\theta(x_t \mid y_t, H_t, H^*) \) and a value function \( V_\phi(y_t, H_t, H^*) \).

% \subsection*{Pretraining (Offline)}

% \textbf{Value Function Pretraining:}

% Given ground-truth data \( \{(y^{(i)}, H^{(i)}, H^{*(i)})\} \), train the value network by minimizing the MSE between its prediction and the immediate reward:

% \[
% L^{\text{VF}}(\phi) = \frac{1}{N} \sum_{i=1}^N \left( V_\phi(y^{(i)}, H^{(i)}, H^{*(i)}) + \text{Error}(H^{(i)}, H^{*(i)}) \right)^2
% \]

% \textbf{Policy Network Pretraining (Optional):}

% If action labels \( x^{(i)} \) are available (from simulation or heuristics), use supervised learning:

% \[
% L^{\text{BC}}(\theta) = -\frac{1}{N} \sum_{i=1}^N \log \pi_\theta(x^{(i)} \mid y^{(i)}, H^{(i)}, H^{*(i)})
% \]

% \subsection*{PPO Training Loop (Online)}

% \begin{enumerate}
%     \item \textbf{Collect Rollouts:} For each episode, do:
%     \begin{align*}
%         y_0 &\leftarrow \text{initial input} \\
%         H_0 &\leftarrow G(y_0) \\
%         \text{for } t = 0 \ldots T: \\
%         &\quad x_t \sim \pi_\theta(x_t \mid y_t, H_t, H^*) \\
%         &\quad y_{t+1} = y_t + x_t \\
%         &\quad H_{t+1} = G(y_{t+1}) \\
%         &\quad r_t = -\text{Error}(H_t, H^*)
%     \end{align*}
    
%     \item \textbf{Compute Value Estimates and Advantages:}
    
%     Let \( V_t = V_\phi(y_t, H_t, H^*) \). Use Generalized Advantage Estimation (GAE):
    
%     \[
%     \delta_t = r_t + \gamma V_{t+1} - V_t
%     \]
    
%     \[
%     \hat{A}_t = \delta_t + \gamma \lambda \hat{A}_{t+1}
%     \]
    
%     \item \textbf{Compute Returns:}
    
%     \[
%     R_t = \sum_{l=0}^\infty \gamma^l r_{t+l}
%     \]
    
%     \item \textbf{Update Value Network:}
    
%     \[
%     L^{\text{VF}}(\phi) = \frac{1}{T} \sum_{t=0}^{T} \left( V_\phi(y_t, H_t, H^*) - R_t \right)^2
%     \]
    
%     \item \textbf{Update Policy Network (PPO Objective):}
    
%     Define the probability ratio:
    
%     \[
%     r_t(\theta) = \frac{\pi_\theta(x_t \mid y_t, H_t, H^*)}{\pi_{\theta_{\text{old}}}(x_t \mid y_t, H_t, H^*)}
%     \]
    
%     The clipped surrogate objective:
    
%     \[
%     L^{\text{CLIP}}(\theta) = \frac{1}{T} \sum_{t=0}^T \min\left(
%     r_t(\theta) \hat{A}_t,\ 
%     \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t
%     \right)
%     \]
    
%     The policy loss to minimize:
    
%     \[
%     L^{\text{POLICY}}(\theta) = -L^{\text{CLIP}}(\theta)
%     \]
    
%     \item \textbf{Repeat.}
% \end{enumerate}


\end{document}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
